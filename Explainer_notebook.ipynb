{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06de880b",
   "metadata": {},
   "source": [
    "# Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2583db41",
   "metadata": {},
   "source": [
    "- What is your dataset?\n",
    "\n",
    "- Why did you choose this/these particular dataset(s)?\n",
    "\n",
    "- What was your goal for the end user's experience?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aff5897",
   "metadata": {},
   "source": [
    "Overall our dataset consists of XXXX amount of theorists wikipedia pages, gathered from five wikipedia lists each representing a science with the overall theme of social science. By using the researchers references to each other we construct edges between the researchers, and therefore are able the map the overall science community of the social sciences. \n",
    "\n",
    "By looking into the researchers from each discipline we hope to find patterns in the connections both within the five categories but also between the five social sciences. We will first seek to answer the question of, what characteristics does the community of the five social sciences have? We will examine this question by looking into the over network properties of each followed by a network analysis. Thereafter we will examine the communities of the social sciences, by doing this we will answer these questions: Are the traditional categories within the social science community actually also apt/suitable for the social science community? Or are the categorizations just a historic relic which we should reject, and then embrace some other maybe more relevant categorizations within the social sciences? By looking into this it becomes possible to either deny or affirm the different scientists relevance within the different disciplines of the social science community. \n",
    "\n",
    "Our overall goal for the end user experience is to give a easy overview of the social sciences. We hope that our community detection models and our HSMB models maybe can give interested people a nuance in their view of the social sciences, instead of keeping to the old disciplines. By looking into our website interested researchers will maybe get inspiration to move out and beyond their own social science. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12260c95",
   "metadata": {},
   "source": [
    "# Basic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc64bbcd",
   "metadata": {},
   "source": [
    "- Write about your choices in data cleaning and preprocessing\n",
    "\n",
    "- Write a short section that discusses the dataset stats (here you can recycle the work you did for Project Assignment A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c36086",
   "metadata": {},
   "source": [
    "### Extracting data from wikipedias API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15fd276",
   "metadata": {},
   "source": [
    "To construct this \"community\" of social scientists, we found inspiration by examining the department for Social Science at Copenhagen University. The Social Science department have five different sciences: Sociology, Psychology, Economy, Anthropology, and Political Science. For each of these five categories we have a list of scientists on wikipedia. We used these five lists of scientists to find and categorise all the scientists in our final dataset. After gathering all the names from the five lists, we then gather each individual scientists wikipage. Thus our data gathering process has two parts: 1) get social scientists from each of the five lists, 2) gather each scientists wikipage. \n",
    "\n",
    "If a scientist is mentioned on more than one list we calssiffy them as being in a sixth category 'Multiple'. \n",
    "\n",
    "Below we go more in depth with each of the two steps and our choices in the data cleaning process. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436952ff",
   "metadata": {},
   "source": [
    "#### Get social scientists from each of the five lists"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6dfb89",
   "metadata": {},
   "source": [
    "Here we called the wiki API for each of the five list and then sorted the data. When cleaning the lists for theorists we used a regular expression to: XXXXXXXXXXXXXXXXX (ESBEN KOM OG FORKLAR) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4baf9594",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################CALLING THE API for the five lists####################################################\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import regex as re \n",
    "import json\n",
    "\n",
    "def get_wiki(_page, get_txt = False):\n",
    "    baseurl = \"https://en.wikipedia.org/w/api.php?\"\n",
    "    action = \"action=query\"\n",
    "    title = f\"titles={_page}\"\n",
    "    content = \"prop=revisions&rvprop=content&rvslots=*\"\n",
    "    dataformat =\"format=json\"\n",
    "    \n",
    "    query = \"{}{}&{}&{}&{}\".format(baseurl, action, content, title, dataformat)\n",
    "    print(query)\n",
    "    \n",
    "    if get_txt == True:\n",
    "        resp = requests.get(query).json()\n",
    "        page_id = [i for i in resp['query']['pages'].keys()][0] # get page id\n",
    "        txt = resp['query']['pages'][page_id]['revisions'][0]['slots']['main']['*']\n",
    "        return txt\n",
    "    \n",
    "    else:\n",
    "        return requests.get(query).json()\n",
    "\n",
    "resp_soc = get_wiki(\"List_of_sociologists\")\n",
    "resp_ant = get_wiki(\"List_of_anthropologists\")\n",
    "resp_pol = get_wiki(\"List_of_political_scientists\")\n",
    "resp_psy = get_wiki(\"List_of_psychologists\")\n",
    "resp_eco = get_wiki(\"List_of_economists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed1a099",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################Cleaning each social scientist discipline###################################\n",
    "''' Soc '''\n",
    "txt = resp_soc['query']['pages']['254243']['revisions'][0]['slots']['main']['*']\n",
    "txt = txt.split('==A==')[1] # drop intro tekst\n",
    "txt = txt.split('==References==')[0] # drop trailing stuff\n",
    "sociologists = re.findall('\\n\\*(?: )?\\[\\[(.*?)(?:\\|.*?)?\\]\\]*',txt)\n",
    "print(f'Sociologists: {len(sociologists)}')\n",
    "\n",
    "''' Ant '''\n",
    "txt = resp_ant['query']['pages']['728']['revisions'][0]['slots']['main']['*']\n",
    "txt = txt.split('==A==')[1] # drop intro tekst\n",
    "txt = txt.split('==Fictional anthropologists==')[0] # drop trailing stuff\n",
    "anthropologists = re.findall('\\n\\*(?: )?\\[\\[(.*?)(?:\\|.*?)?\\]\\]*',txt)\n",
    "print(f'Anthropologist: {len(anthropologists)}')\n",
    "\n",
    "''' Eco '''\n",
    "txt = resp_eco[\"query\"]['pages']['10231']['revisions'][0]['slots']['main']['*']\n",
    "txt = txt.split('==A==')[1] # drop intro tekst\n",
    "txt = txt.split('==See also==')[0] # drop trailing stuff\n",
    "economists = re.findall('\\n\\*(?: )?\\[\\[(.*?)(?:\\|.*?)?\\]\\]*',txt)\n",
    "print(f'economists: {len(economists)}')\n",
    "\n",
    "''' Psy '''\n",
    "txt = resp_psy['query']['pages']['199877']['revisions'][0]['slots']['main']['*']\n",
    "txt = txt.split('== A ==')[1] # drop intro tekst\n",
    "txt = txt.split('==See also==')[0] # drop trailing stuff\n",
    "psychologists = re.findall('\\n\\*(?: )?\\[\\[(.*?)(?:\\|.*?)?\\]\\]*',txt)\n",
    "print(f'Psychologists: {len(psychologists)}')\n",
    "\n",
    "''' Pol '''\n",
    "txt = resp_pol['query']['pages']['37559']['revisions'][0]['slots']['main']['*']\n",
    "txt = txt.split('== A ==')[1] # drop intro tekst\n",
    "txt = txt.split('== See also ==')[0] # drop trailing stuff\n",
    "political_scientists = re.findall('\\n\\*(?: )?\\[\\[(.*?)(?:\\|.*?)?\\]\\]*',txt)\n",
    "print(f'Political_scientists: {len(political_scientists)}')\n",
    "\n",
    "# To dict\n",
    "science_dict = {'soc':sociologists,\n",
    "                'anth':anthropologists,\n",
    "                'eco': economists,\n",
    "                'psy': psychologists,\n",
    "                'pol': political_scientists}\n",
    "\n",
    "with open('science_name_dict.json', 'w', encoding = 'utf-8') as f:\n",
    "    json.dump(science_dict,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64390b2",
   "metadata": {},
   "source": [
    "#### Gather each scientists wikipage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27cc3267",
   "metadata": {},
   "source": [
    "For gathering the theorists wikipages we use the \"get_wiki\" function, which we also used to gather the fives lists before. But when gatherings XXXX amount of links instead of five it is also natural to meet some more problems. We both had the problem that some theorists pages redirected us to another page, which we need to call  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f153389b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import copy\n",
    "\n",
    "with open('science_name_dict.json', 'r', encoding = 'utf-8') as f:\n",
    "    science_dict = json.load(f)\n",
    "\n",
    "# Copy dict - deep copy as the values are nested in a list\n",
    "new_science_dict = copy.deepcopy(science_dict)\n",
    "\n",
    "    \n",
    "''' Scrape the pages '''\n",
    "for field, nodes in science_dict.items():\n",
    "    for node in nodes:\n",
    "        time.sleep(0.2)\n",
    "        # A very few times it shows uses either a # or ?, and this always redirects to topic, \n",
    "        # We therefore skip it. An example is: 'Karen_Horney#Ten_neurotic_needs'\n",
    "        if '#' in node or '?' in node: \n",
    "            new_science_dict[field].remove(node) # remove node \n",
    "            print('Skipping: ', node)\n",
    "            continue\n",
    "            \n",
    "        node = node.replace(' ', '_')\n",
    "        try:\n",
    "            txt = get_wiki(node, get_txt = True)\n",
    "            \n",
    "            # Sometimes the page link we have gotten from one of our five lists is a \"redirect\"\n",
    "            # Here it is nessecary to extract this new one. \n",
    "            if \"#REDIRECT\" in txt: # if it is a redirect\n",
    "                new_science_dict[field].remove(node.replace('_', ' '))  # delete the old value \n",
    "                node = [re.findall('\\[\\[(.*?)(?:\\|.*?)?\\]\\]', txt)[0]] # redirected link \n",
    "                print(f\"## Redirect! {node}\")\n",
    "                new_science_dict[field] += node # append the new node (only element in a list) to the science_dict \n",
    "                node = node[0].replace(' ', '_') # extract from the list and replace \n",
    "                \n",
    "                if '#' in node or '?' in node: \n",
    "                    new_science_dict[field].remove(node.replace('_', ' ')) # remove from dict\n",
    "                    print('Skipping: ', node)\n",
    "                    continue                    \n",
    "                    \n",
    "                txt = get_wiki(node, get_txt=True)\n",
    "                \n",
    "        except KeyError as e: # if the page is incomplete (red hyperlinks)\n",
    "            new_science_dict[field].remove(node.replace('_', ' ')) # remove from dict \n",
    "            print(e, node)\n",
    "            continue\n",
    "        \n",
    "        with open(f'wiki_content/{node}.txt', 'w', encoding = 'utf-8') as f:\n",
    "            f.write(txt)\n",
    "            \n",
    "# Gem den opdaterede dict \n",
    "with open('science_name_dict.json', 'w', encoding = 'utf-8') as f:\n",
    "    json.dump(new_science_dict,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95782e65",
   "metadata": {},
   "source": [
    "#### Parsing the data for constructing the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e602c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for field, nodes in new_science_dict.items():\n",
    "    for node in nodes:\n",
    "        node = node.replace(' ','_')\n",
    "        with open(f'wiki_content/{node}.txt', 'r', encoding = 'utf-8') as f:\n",
    "            f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa62e78",
   "metadata": {},
   "source": [
    "#### Parsing the data for text analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faabd82a",
   "metadata": {},
   "source": [
    "# Tools, theory and analysis. Describe the process of theory to insight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572fe810",
   "metadata": {},
   "source": [
    "- Talk about how you've worked with text, including regular expressions, unicode, etc.\n",
    "Describe which network science tools and data analysis strategies you've used, how those network science measures work, and why the tools you've chosen are right for the problem you're solving.\n",
    "\n",
    "- How did you use the tools to understand your dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de6ddc6",
   "metadata": {},
   "source": [
    "#### ANALYSE STRUKTUR: \n",
    "Explain the overall idea\n",
    "\n",
    "- Analysis step 1\n",
    "    - explain what you're interested in\n",
    "    - explain the tool\n",
    "    - apply the tool\n",
    "    - discuss the outcome\n",
    "\n",
    "- Analysis step 2\n",
    "    - explain what you're interested in\n",
    "    - explain the tool\n",
    "    - apply the tool\n",
    "    - discuss the outcome\n",
    "\n",
    "- Analysis step 3,\n",
    "... and so on until the analysis is done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e3bff6",
   "metadata": {},
   "source": [
    "# Discussion. Think critically about your creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de25137",
   "metadata": {},
   "source": [
    "- What went well?,\n",
    "- What is still missing? What could be improved?, Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3745a89e",
   "metadata": {},
   "source": [
    "# Constributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f60c4c",
   "metadata": {},
   "source": [
    "- You should write (just briefly) which group member was the main responsible for which elements of the assignment. (I want you guys to understand every part of the assignment, but usually there is someone who took lead role on certain portions of the work. That’s what you should explain).\n",
    "\n",
    "- It is not OK simply to write \"All group members contributed equally\"."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
