{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What is your dataset?\n",
    "\n",
    "- Why did you choose this/these particular dataset(s)?\n",
    "\n",
    "- What was your goal for the end user's experience?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall our dataset consists of XXXX amount of theorists wikipedia pages, gathered from five wikipedia lists each representing a science with the overall theme of social science. By using the researchers references to each other we construct edges between the researchers, and therefore are able the map the overall science community of the social sciences. \n",
    "\n",
    "By looking into the researchers from each discipline we hope to find patterns in the connections both within the five categories but also between the five social sciences. We will first seek to answer the question of, what characteristics does the community of the five social sciences have? We will examine this question by looking into the over network properties of each followed by a network analysis. Thereafter we will examine the communities of the social sciences, by doing this we will answer these questions: Are the traditional categories within the social science community actually also apt/suitable for the social science community? Or are the categorizations just a historic relic which we should reject, and then embrace some other maybe more relevant categorizations within the social sciences? By looking into this it becomes possible to either deny or affirm the different scientists relevance within the different disciplines of the social science community. \n",
    "\n",
    "Our overall goal for the end user experience is to give a easy overview of the social sciences. We hope that our community detection models and our HSMB models maybe can give interested people a nuance in their view of the social sciences, instead of keeping to the old disciplines. By looking into our website interested researchers will maybe get inspiration to move out and beyond their own social science. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Write about your choices in data cleaning and preprocessing\n",
    "\n",
    "- Write a short section that discusses the dataset stats (here you can recycle the work you did for Project Assignment A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting data from wikipedias API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To construct this \"community\" of social scientists, we found inspiration by examining the department for Social Science at Copenhagen University. The Social Science department have five different sciences: Sociology, Psychology, Economy, Anthropology, and Political Science. For each of these five categories we have a list of scientists on wikipedia. We used these five lists of scientists to find and categorise all the scientists in our final dataset. After gathering all the names from the five lists, we then gather each individual scientists wikipage. Thus our data gathering process has two parts: 1) get social scientists from each of the five lists, 2) gather each scientists wikipage. \n",
    "\n",
    "If a scientist is mentioned on more than one list we calssiffy them as being in a sixth category 'Multiple'. \n",
    "\n",
    "Below we go more in depth with each of the two steps and our choices in the data cleaning process. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get social scientists from each of the five lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we called the wiki API for each of the five list and then sorted the data. When cleaning the lists for theorists we used a regular expression to: XXXXXXXXXXXXXXXXX (ESBEN KOM OG FORKLAR) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################CALLING THE API for the five lists####################################################\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import regex as re \n",
    "import json\n",
    "\n",
    "def get_wiki(_page, get_txt = False):\n",
    "    baseurl = \"https://en.wikipedia.org/w/api.php?\"\n",
    "    action = \"action=query\"\n",
    "    title = f\"titles={_page}\"\n",
    "    content = \"prop=revisions&rvprop=content&rvslots=*\"\n",
    "    dataformat =\"format=json\"\n",
    "    \n",
    "    query = \"{}{}&{}&{}&{}\".format(baseurl, action, content, title, dataformat)\n",
    "    print(query)\n",
    "    \n",
    "    if get_txt == True:\n",
    "        resp = requests.get(query).json()\n",
    "        page_id = [i for i in resp['query']['pages'].keys()][0] # get page id\n",
    "        txt = resp['query']['pages'][page_id]['revisions'][0]['slots']['main']['*']\n",
    "        return txt\n",
    "    \n",
    "    else:\n",
    "        return requests.get(query).json()\n",
    "\n",
    "resp_soc = get_wiki(\"List_of_sociologists\")\n",
    "resp_ant = get_wiki(\"List_of_anthropologists\")\n",
    "resp_pol = get_wiki(\"List_of_political_scientists\")\n",
    "resp_psy = get_wiki(\"List_of_psychologists\")\n",
    "resp_eco = get_wiki(\"List_of_economists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################Cleaning each social scientist discipline###################################\n",
    "''' Soc '''\n",
    "txt = resp_soc['query']['pages']['254243']['revisions'][0]['slots']['main']['*']\n",
    "txt = txt.split('==A==')[1] # drop intro tekst\n",
    "txt = txt.split('==References==')[0] # drop trailing stuff\n",
    "sociologists = re.findall('\\n\\*(?: )?\\[\\[(.*?)(?:\\|.*?)?\\]\\]*',txt)\n",
    "print(f'Sociologists: {len(sociologists)}')\n",
    "\n",
    "''' Ant '''\n",
    "txt = resp_ant['query']['pages']['728']['revisions'][0]['slots']['main']['*']\n",
    "txt = txt.split('==A==')[1] # drop intro tekst\n",
    "txt = txt.split('==Fictional anthropologists==')[0] # drop trailing stuff\n",
    "anthropologists = re.findall('\\n\\*(?: )?\\[\\[(.*?)(?:\\|.*?)?\\]\\]*',txt)\n",
    "print(f'Anthropologist: {len(anthropologists)}')\n",
    "\n",
    "''' Eco '''\n",
    "txt = resp_eco[\"query\"]['pages']['10231']['revisions'][0]['slots']['main']['*']\n",
    "txt = txt.split('==A==')[1] # drop intro tekst\n",
    "txt = txt.split('==See also==')[0] # drop trailing stuff\n",
    "economists = re.findall('\\n\\*(?: )?\\[\\[(.*?)(?:\\|.*?)?\\]\\]*',txt)\n",
    "print(f'economists: {len(economists)}')\n",
    "\n",
    "''' Psy '''\n",
    "txt = resp_psy['query']['pages']['199877']['revisions'][0]['slots']['main']['*']\n",
    "txt = txt.split('== A ==')[1] # drop intro tekst\n",
    "txt = txt.split('==See also==')[0] # drop trailing stuff\n",
    "psychologists = re.findall('\\n\\*(?: )?\\[\\[(.*?)(?:\\|.*?)?\\]\\]*',txt)\n",
    "print(f'Psychologists: {len(psychologists)}')\n",
    "\n",
    "''' Pol '''\n",
    "txt = resp_pol['query']['pages']['37559']['revisions'][0]['slots']['main']['*']\n",
    "txt = txt.split('== A ==')[1] # drop intro tekst\n",
    "txt = txt.split('== See also ==')[0] # drop trailing stuff\n",
    "political_scientists = re.findall('\\n\\*(?: )?\\[\\[(.*?)(?:\\|.*?)?\\]\\]*',txt)\n",
    "print(f'Political_scientists: {len(political_scientists)}')\n",
    "\n",
    "# To dict\n",
    "science_dict = {'soc':sociologists,\n",
    "                'anth':anthropologists,\n",
    "                'eco': economists,\n",
    "                'psy': psychologists,\n",
    "                'pol': political_scientists}\n",
    "\n",
    "with open('science_name_dict.json', 'w', encoding = 'utf-8') as f:\n",
    "    json.dump(science_dict,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gather each scientists wikipage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For gathering the theorists wikipages we use the \"get_wiki\" function, which we also used to gather the fives lists before. But when gatherings XXXX amount of links instead of five it is also natural to meet some more problems. We both had the problem that some theorists pages redirected us to another page, which we need to call  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import copy\n",
    "\n",
    "with open('science_name_dict.json', 'r', encoding = 'utf-8') as f:\n",
    "    science_dict = json.load(f)\n",
    "\n",
    "# Copy dict - deep copy as the values are nested in a list\n",
    "new_science_dict = copy.deepcopy(science_dict)\n",
    "\n",
    "    \n",
    "''' Scrape the pages '''\n",
    "for field, nodes in science_dict.items():\n",
    "    for node in nodes:\n",
    "        time.sleep(0.2)\n",
    "        # A very few times it shows uses either a # or ?, and this always redirects to topic, \n",
    "        # We therefore skip it. An example is: 'Karen_Horney#Ten_neurotic_needs'\n",
    "        if '#' in node or '?' in node: \n",
    "            new_science_dict[field].remove(node) # remove node \n",
    "            print('Skipping: ', node)\n",
    "            continue\n",
    "            \n",
    "        node = node.replace(' ', '_')\n",
    "        try:\n",
    "            txt = get_wiki(node, get_txt = True)\n",
    "            \n",
    "            # Sometimes the page link we have gotten from one of our five lists is a \"redirect\"\n",
    "            # Here it is nessecary to extract this new one. \n",
    "            if \"#REDIRECT\" in txt: # if it is a redirect\n",
    "                new_science_dict[field].remove(node.replace('_', ' '))  # delete the old value \n",
    "                node = [re.findall('\\[\\[(.*?)(?:\\|.*?)?\\]\\]', txt)[0]] # redirected link \n",
    "                print(f\"## Redirect! {node}\")\n",
    "                new_science_dict[field] += node # append the new node (only element in a list) to the science_dict \n",
    "                node = node[0].replace(' ', '_') # extract from the list and replace \n",
    "                \n",
    "                if '#' in node or '?' in node: \n",
    "                    new_science_dict[field].remove(node.replace('_', ' ')) # remove from dict\n",
    "                    print('Skipping: ', node)\n",
    "                    continue                    \n",
    "                    \n",
    "                txt = get_wiki(node, get_txt=True)\n",
    "                \n",
    "        except KeyError as e: # if the page is incomplete (red hyperlinks)\n",
    "            new_science_dict[field].remove(node.replace('_', ' ')) # remove from dict \n",
    "            print(e, node)\n",
    "            continue\n",
    "        \n",
    "        with open(f'wiki_content/{node}.txt', 'w', encoding = 'utf-8') as f:\n",
    "            f.write(txt)\n",
    "            \n",
    "# Gem den opdaterede dict \n",
    "with open('science_name_dict.json', 'w', encoding = 'utf-8') as f:\n",
    "    json.dump(new_science_dict,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing the data for constructing the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the name of all the files from the wiki-scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import regex as re \n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import json \n",
    "\n",
    "# get files in directory \n",
    "onlyfiles = [f for f in listdir(\"wiki_content\") if isfile(join(\"wiki_content\", f))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make link list to construct a network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Extract links '''\n",
    "theo_dict  = {}\n",
    "for file in onlyfiles: # loop over the files \n",
    "    with open(f'wiki_content/{file}','r', encoding = 'utf-8') as f:\n",
    "        txt = f.read() # read files \n",
    "        links = re.findall('\\[\\[(.*?)(?:\\|.*?)?]]', txt) # find links\n",
    "        name = file.split('.txt')[0].replace('_',' ') # get name of the scientist\n",
    "        theo_dict.update({name:links}) # update dict with scientist:all_links\n",
    "\n",
    "''' WHo links to who '''\n",
    "# dict with scientist:link_to_other_scientists\n",
    "# .replace('_', ' ') on names, because some names are weird like 'Anselm_Strauss'\n",
    "link_dict = {theorist:[_name.replace('_', ' ') for _name in links \\\n",
    "                       if _name.replace('_', ' ') in theo_dict.keys()]\\\n",
    "                       for theorist, links in theo_dict.items()}\n",
    "\n",
    "''' To Link list '''\n",
    "# Convert to list of (node_a, node_b)-set\n",
    "link_list = [(node_a, node_b) for node_a, nodes in link_dict.items() for node_b in nodes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the dictionary `science_dict` to a structure where scientists are keys, and their dicipline is the value. If a scientist appears in more than one of the lists from wikipedia, we categorize them as `multiple`. The new dictionary will be used a attributes in the network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Get science attribute '''\n",
    "with open('science_name_dict.json', 'r', encoding = 'utf-8') as f: \n",
    "    science_dict = json.load(f)\n",
    "    \n",
    "''' Get those that are in multiple scientific field lists '''\n",
    "\n",
    "socs = set(science_dict['soc'])\n",
    "anths = set(science_dict['anth'])\n",
    "ecos = set(science_dict['eco'])\n",
    "psys = set(science_dict['psy'])\n",
    "pols = set(science_dict['pol'])\n",
    "\n",
    "# See of they appear in multiple field lists\n",
    "seen = []\n",
    "repeated = []\n",
    "for l in [socs, anths, ecos, psys, pols]:\n",
    "    for i in l: # loop over de enkelte guys'n'gals\n",
    "        if i in seen:\n",
    "            repeated.append(i)\n",
    "        else:\n",
    "            seen.append(i)\n",
    "\n",
    "''' Inverse the dict to {scientist:field}-structure '''\n",
    "inv_science_dict = {name:science for science, names in science_dict.items() for name in names}\n",
    "''' change the scientific field to \"both\" if they are both in more than one field list '''\n",
    "inv_science_dict = {name:('multiple' if name in repeated else sci)\\\n",
    "                    for name, sci in inv_science_dict.items()}\n",
    "\n",
    "# Match keys to link_dict --> when importing link_dict, 'Anselm_Strauss' becomes 'Anselm Strauss'\n",
    "# but in the inv_science_dict he is 'Anselm_Strauss'. So it is a mess without it   \n",
    "inv_science_dict = {key.replace('_', ' '):val for key, val in inv_science_dict.items()}\n",
    "\n",
    "with open('inv_science_name_dict.json', 'w', encoding = 'utf-8') as f: \n",
    "    json.dump(inv_science_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Make network '''\n",
    "import networkx as nx\n",
    "import netwulf as nw\n",
    "G = nx.DiGraph()\n",
    "G.add_nodes_from(link_dict.keys())\n",
    "G.add_edges_from(link_list)\n",
    "nx.set_node_attributes(G, inv_science_dict, name = 'group') # node attributes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the network using netwulf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Plot network '''\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-white')\n",
    "\n",
    "network, config = nw.visualize(G, plot_in_cell_below=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drawing the network\n",
    "fig, ax = nw.draw_netwulf(network)\n",
    "\n",
    "# assigning labels only to the largest nodes \n",
    "for node_nr in range(len(network['nodes'])):\n",
    "    if network['nodes'][node_nr]['radius']>7:\n",
    "        nw.tools.add_node_label(ax,\n",
    "                                network,\n",
    "                                dy = 12,\n",
    "                                node_id = network['nodes'][node_nr]['id'],\n",
    "                                size=network['nodes'][node_nr]['radius']*5)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "# Drawing a big figure\n",
    "plt.rcParams[\"figure.figsize\"] = (30,30)\n",
    "\n",
    "val_map = {inv_science_dict[node['id']]:node['color'] for node in network['nodes']}\n",
    "\n",
    "# Add legends\n",
    "for label in val_map.keys():\n",
    "    ax.plot([],[],color=val_map[label], label=label, marker='o', linestyle=\"None\", markersize = 20)\n",
    "\n",
    "plt.legend(fontsize = 25)\n",
    "ax.set_title('Network of Social Scientists on Wikipedia', size = 40)\n",
    "# Saving as pdf\n",
    "plt.savefig('Plots/field_network_title.png', dpi=None, facecolor='w', edgecolor='w', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the configuration for plotting the same network later, but with colors based on the community partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Save the network configurations '''\n",
    "with open(\"network_configurations/big_network.json\", 'w') as f:\n",
    "    json.dump(network, f)\n",
    "    \n",
    "with open(\"network_configurations/big_config.json\", 'w') as f:\n",
    "    json.dump(config, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' load the network configurations '''\n",
    "with open(\"network_configurations/big_network.json\", 'r') as f:\n",
    "    network = json.load(f)\n",
    "\n",
    "with open(\"network_configurations/big_config.json\", 'r') as f:\n",
    "    config = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriptive statistics on the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-white')\n",
    "\n",
    "''' Function to plot Degree distribution '''\n",
    "def plot_deg_dist(deg_seq, title = 'Degree distribution', save = False, save_title = \"\", kwgs = {}):\n",
    "    plt.style.use('ggplot')\n",
    "    \n",
    "    fig, ax = plt.subplots(2,1, figsize=(10,8))\n",
    "    \n",
    "    sns.scatterplot(x=deg_seq.keys(), y=deg_seq.values(), ax = ax[0], **kwgs)\n",
    "    ax[0].set_title(title)\n",
    "    ax[0].set_ylabel('Frequency', size = 15)\n",
    "    \n",
    "    del deg_seq[0] # messes up the log plot so remove 0 deg nodes \n",
    "    sns.scatterplot(x=deg_seq.keys(), y=deg_seq.values(), ax = ax[1], **kwgs)\n",
    "\n",
    "    ax[1].set_title(title + ' (log scaled)')\n",
    "    ax[1].set_ylabel('Frequency', size = 15)\n",
    "    ax[1].set_xlabel('k', size = 15)\n",
    "    # logscale the axes\n",
    "    ax[1].set_yscale('log')\n",
    "    ax[1].set_xscale('log')\n",
    "    \n",
    "    # Saving as pdf\n",
    "    if save = True:\n",
    "        plt.savefig(f'Plots/{save_title}.png', dpi=200, facecolor='w', edgecolor='w', bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Plot degree distribution '''\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "# Get in and out degree\n",
    "in_deg_dist = Counter([d for n,d in G.in_degree()])\n",
    "out_deg_dist = Counter([d for n,d in G.out_degree()])\n",
    "\n",
    "# Plot and save \n",
    "plot_deg_dist(in_deg_dist, 'In-degree Distribution', save=False, save_title=\"in_deg_dist\")\n",
    "plot_deg_dist(out_deg_dist, 'Out-degree Distribution', save=False, save_title= \"out_deg_dist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the most connected nodes based on in- and out- degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Plot the top X out- and in- degree nodes '''\n",
    "def plot_top_nodes(top_n = 50, in_deg = True, save=False, save_title = \"\"):\n",
    "    if in_deg:\n",
    "        top_nodes = sorted(dict(G.in_degree()).items(), key = lambda x: x[1], reverse = True)[:top_n]\n",
    "    else:\n",
    "        top_nodes = sorted(dict(G.out_degree()).items(), key = lambda x: x[1], reverse = True)[:top_n]\n",
    "    x = [_[0] for _ in top_nodes]\n",
    "    y = [_[1] for _ in top_nodes]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize = (12,6))\n",
    "    sns.scatterplot(x,y, ax=ax)\n",
    "    ax.set_ylabel('Degree', size = 15)\n",
    "    plt.xticks(rotation=90, size = 12)\n",
    "    if in_deg:\n",
    "        ax.set_title(f'Distribution of in-degree for top {top_n}', size = 15)\n",
    "    else:\n",
    "        ax.set_title(f'Distribution of out-degree for top {top_n}', size = 15)\n",
    "        \n",
    "    # Saving as pdf\n",
    "    if save = True:\n",
    "        plt.savefig(f'Plots/{save_title}.png', dpi=200, facecolor='w', edgecolor='w', bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Call function '''\n",
    "plot_top_nodes(in_deg = True, save_title=\"top_in_deg\")\n",
    "plot_top_nodes(in_deg = False, save_title=\"top_out_deg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Communities\n",
    "\n",
    "Now that we have present some basic statistic of the network, we will will examine how and which communities are formed in the network based on the modularity optimization of the Louvain Partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import community\n",
    "\n",
    "''' Only the GCC, otherwise there will be about 300+ communities '''\n",
    "G_c = G.copy()\n",
    "c = max(nx.weakly_connected_components(G_c), key=len) # largest connected component\n",
    "GCC = G_c.subgraph(c).copy() # make subgraph of the largest connected component\n",
    "G_undir = GCC.to_undirected().copy() # undirected\n",
    "\n",
    "# Louvain modularity for partition\n",
    "partition = community.best_partition(G_undir, random_state = 280395)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dict with name as key and field and community dicts as values \n",
    "joined_dict = {name:{'field':inv_science_dict[name], 'community':com}\\\n",
    "               for name, com in partition.items()}\n",
    "\n",
    "with open('name_field_community.json', 'w', encoding = 'utf-8') as f: \n",
    "    json.dump(joined_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Plot community partition '''\n",
    "nx.set_node_attributes(G_undir, partition, name = 'group') # node attributes \n",
    "## Use the configs from the big non-GCC network\n",
    "network_part, config_part = nw.visualize(G_undir, plot_in_cell_below=False, config = config)\n",
    "\n",
    "fig, ax = nw.draw_netwulf(network_part, figsize = 10)\n",
    "\n",
    "# {community:color} dict for the legend\n",
    "val_map = {partition[node['id']]:node['color'] for node in network_part['nodes']}\n",
    "\n",
    "# Add legends\n",
    "for v in sorted(val_map.keys()):\n",
    "    plt.scatter([],[],color=val_map[v], \n",
    "                label='Community {}'.format(v), \n",
    "                marker='o',\n",
    "                linestyle=\"None\")\n",
    "# Place legend\n",
    "plt.legend(bbox_to_anchor = (1,0.8))\n",
    "# Saving as pdf\n",
    "fig.savefig(f'Plots/community_network.png', dpi=200, facecolor='w', edgecolor='w', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parsing the data for text analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tools, theory and analysis. Describe the process of theory to insight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Talk about how you've worked with text, including regular expressions, unicode, etc.\n",
    "Describe which network science tools and data analysis strategies you've used, how those network science measures work, and why the tools you've chosen are right for the problem you're solving.\n",
    "\n",
    "- How did you use the tools to understand your dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ANALYSE STRUKTUR: \n",
    "Explain the overall idea\n",
    "\n",
    "- Analysis step 1\n",
    "    - explain what you're interested in\n",
    "    - explain the tool\n",
    "    - apply the tool\n",
    "    - discuss the outcome\n",
    "\n",
    "- Analysis step 2\n",
    "    - explain what you're interested in\n",
    "    - explain the tool\n",
    "    - apply the tool\n",
    "    - discuss the outcome\n",
    "\n",
    "- Analysis step 3,\n",
    "... and so on until the analysis is done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion. Think critically about your creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What went well?,\n",
    "- What is still missing? What could be improved?, Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You should write (just briefly) which group member was the main responsible for which elements of the assignment. (I want you guys to understand every part of the assignment, but usually there is someone who took lead role on certain portions of the work. That’s what you should explain).\n",
    "\n",
    "- It is not OK simply to write \"All group members contributed equally\"."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
